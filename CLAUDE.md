# Japanese Text Vector - Project Overview

## ğŸ¯ Project Purpose

A lightweight WASM-compatible Japanese text vectorization library using TF-IDF + LSA (Latent Semantic Analysis) to calculate semantic similarity of Japanese documents in real-time within browsers.

## ğŸ—ï¸ Architecture

### Core Technologies

- **Language**: Rust + WebAssembly (WASM)
- **Algorithm**: TF-IDF + LSA (dimensionality reduction via SVD)
- **Japanese Processing**: Character N-gram + character type detection for morphological analysis-free design

### Module Structure

```
src/
â”œâ”€â”€ lib.rs              # WASM exports
â”œâ”€â”€ tokenizer.rs        # Japanese tokenizer
â”œâ”€â”€ tfidf_lsa.rs       # TF-IDF + LSA implementation
â”œâ”€â”€ incremental.rs      # Incremental learning features
â”œâ”€â”€ stable_hash.rs      # Hash-based stable vectorization
â””â”€â”€ utils.rs           # Utility functions
```

## ğŸ’¡ Key Features

### 1. IncrementalEmbedder (Incremental Learning Version)

- **Dynamic Model Updates**: Automatically retrains model as documents are added
- **Duplicate Detection**: Automatic exclusion of identical documents using HashSet
- **Background Processing**: Non-blocking retraining via step_retrain()
- **Persistence**: Model save/restore in JSON format

### 2. StableHashEmbedder (Stable Hash Version)

- **Stability**: Fixed vectorization unaffected by document additions
- **High Performance**: Instant vector generation using hash functions
- **Memory Efficient**: Lightweight implementation without learning models

### 3. Japanese Tokenizer

- **Character N-grams**: Combination of 2-grams and 3-grams
- **Character Type Detection**: Identification of hiragana, katakana, kanji, alphanumeric
- **Word Boundary Estimation**: Simple word segmentation based on character type changes
- **Vocabulary Limiting**: Document frequency filtering (max 50,000 words)

## ğŸ“Š Performance Metrics

| Metric | Achievement |
|--------|------------|
| WASM Size | **182KB** (with high-accuracy SVD) |
| Vectorization Speed | ~5ms/document |
| Embedding Dimensions | 64-128 (configurable) |
| Memory Usage | Minimal (using wee_alloc) |
| SVD Implementation | Full nalgebra SVD for high accuracy |

## ğŸ¨ Demo Pages

### 1. basic_usage.html

- Basic usage examples
- Try both StableHashEmbedder and IncrementalEmbedder
- Similarity calculation and batch search features

### 2. incremental_demo.html

- **300 sample documents** (15 categories Ã— 20 documents)
- Real-time incremental learning visualization
- Duplicate detection demonstration
- Performance metrics display

## ğŸ”§ Technical Optimizations

### Memory Optimization

- Lightweight allocator using `wee_alloc`
- Size optimization with `opt-level = "s"`
- Efficient sparse matrix processing

### Japanese-Specific Design

- No morphological analysis required (no dictionary dependencies like MeCab)
- Semantic boundary estimation using character types
- Word segmentation through particle detection

### WASM Integration

- Conditional compilation (`#[cfg_attr(target_arch = "wasm32", wasm_bindgen)]`)
- Support for testing on non-WASM targets
- Automatic JavaScript API generation

## ğŸ“ˆ Future Extensibility

### Implemented

- âœ… Basic TF-IDF + LSA
- âœ… Incremental learning functionality
- âœ… Automatic duplicate document detection
- âœ… Model persistence
- âœ… 300 sample documents

### Planned

- â³ WebGPU support for acceleration
- â³ Advanced dimensionality reduction methods (t-SNE, UMAP)
- â³ Customizable tokenizer
- â³ Streaming processing support

## ğŸš€ Build & Test

```bash
# Run Rust tests (all 19 tests passing)
cargo test

# Build WASM
wasm-pack build --target web --out-dir pkg

# Launch demo page
python3 -m http.server 8000
# Navigate to http://localhost:8000/examples/incremental_demo.html
```

## ğŸ“ Usage Example

```javascript
// IncrementalEmbedder - Incremental learning version
const embedder = new IncrementalEmbedder(0.3);

// Add documents (duplicates automatically excluded)
embedder.add_document("ä»Šæ—¥ã¯å¤©æ°—ãŒã„ã„ã§ã™ã­", 64);
embedder.add_document("ä»Šæ—¥ã¯å¤©æ°—ãŒã„ã„ã§ã™ã­", 64); // Skipped

// Vectorization
const vector = embedder.transform("ä»Šæ—¥ã¯æ™´ã‚Œã§ã™");

// Calculate similarity
const similarity = embedder.get_similarity(
    "ä»Šæ—¥ã¯å¤©æ°—ãŒã„ã„",
    "æ˜æ—¥ã¯å¤©æ°—ãŒæ‚ªã„"
);

// Check document existence
if (!embedder.contains_document(text)) {
    embedder.add_document(text, 64);
}
```

## ğŸ¯ Project Strengths

1. **Lightweight**: Minimal browser load at just 152KB
2. **Fast**: Real-time vectorization and similarity calculation
3. **Japanese-Optimized**: Dictionary-free optimization for Japanese characteristics
4. **Flexible**: Two implementations - incremental learning and stable version
5. **Practical**: Rich features including duplicate detection, persistence, and background processing

## ğŸ“Œ Limitations

- Simplified LSA implementation may reduce accuracy with large-scale data
- Document count limited by browser memory constraints
- Character-based processing makes homophone distinction difficult

## ğŸ” Debugging Tips

- Check WASM errors in browser console
- Use `contains_document()` for duplicate checking
- Verify actual document count with `get_unique_document_count()`
- Monitor memory usage in Performance tab

## ğŸ”¬ Technical Details

### Why TF-IDF + LSA?

- **TF-IDF**: Captures term importance across document collection
- **LSA**: Reduces dimensionality while preserving semantic relationships
- **Combination**: Balances computational efficiency with semantic accuracy
- **High-Accuracy SVD**: Using nalgebra's full SVD implementation for precise latent semantic analysis
  - Singular value weighting for better representation
  - Preserves important latent dimensions
  - Improved similarity calculations

### Incremental Learning Strategy

1. **Threshold-based triggering**: Retrains when change ratio exceeds threshold
2. **Non-blocking execution**: Uses step-by-step processing to avoid UI freezing
3. **Model swapping**: Seamlessly switches to new model once training completes

### Japanese Language Handling

- **No dictionary required**: Uses character patterns instead of morphological analysis
- **Multi-granularity**: Combines different n-gram sizes for better coverage
- **Character type awareness**: Leverages Japanese script transitions for word boundaries

## ğŸŒŸ Unique Aspects

This project stands out by providing a fully client-side Japanese text analysis solution that:

- Runs entirely in the browser without server dependencies
- Handles Japanese text without external dictionaries
- Adapts to new documents dynamically
- Maintains sub-200KB footprint
- Provides both stable and adaptive vectorization strategies

The combination of these features makes it ideal for privacy-conscious applications, offline-capable PWAs, and real-time text analysis scenarios where server round-trips are undesirable.
